
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>(04) PyTorch profiler &#8212; MT/MST project</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="(06) Sim – kNN analysis" href="06_sim-kNN.html" />
    <link rel="prev" title="(04) Dan notebook" href="04_dan.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">MT/MST project</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../md/intro.html">
   Welcome
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  January 2022
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../md/2022-01.html">
   Summary
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../md/2022-01_updates.html">
   Updates
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2022-01/jan-23-2022.html">
     Jan 23rd, 2022 (jb tutorial)
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  October 2022
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../md/2022-10.html">
   Summary
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../md/2022-10_updates.html">
   Updates
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2022-10/oct-01-2022.html">
     Oct 1st, 2022 (Load Wild Data)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2022-10/oct-06-2022.html">
     Oct 6th, 2022 (Python processed)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2022-10/oct-07-2022.html">
     Oct 7th, 2022 (Linear: STA – draft)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2022-10/oct-08-2022.html">
     Oct 8th, 2022 (Linear: STA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2022-10/oct-09-2022.html">
     Oct 9th, 2022 (Opticflow stats)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2022-10/oct-11-2022.html">
     Oct 11th, 2022 (Rot Conv verify)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2022-10/oct-16-2022.html">
     Oct 16th, 2022 (Linear: GLM – spatial)
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  January 2023
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../md/2023-01.html">
   Summary
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../md/2023-01_updates.html">
   Updates
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-01/08_wild.html">
     (08) Wild play
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-01/10_nardin.html">
     (10) Nardin play
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-01/15_sim-draft.html">
     (15) Simulation – draft
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-01/16_sim-wrong.html">
     (16) Simulation – wrong
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-01/21_sim-explore.html">
     (21) Simulation – explore
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-01/23_sim-self-draft.html">
     (23) Simulation – self-motion aspect [draft]
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-01/24_sim-self-draft.html">
     (24) Simulation – self [still draft]
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-01/28_sim-self.html">
     (28) Simulation – self [velocity: new way]
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-01/29_sim-full-draft.html">
     (29) Simulation – full [draft]
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-01/30_sim-full-final.html">
     (30) Simulation – full [final]
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  February 2023
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../md/2023-02.html">
   Summary
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../md/2023-02_updates.html">
   Updates
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-02/01_play-nvae.html">
     (01) NVAE – play
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-02/02_play-vae2d.html">
     (02) VAE2D – play
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-02/09_rot-conv.html">
     (09) RotConv2d – verified
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-02/23_sim-gen.html">
     (23) Simulation – generate
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-02/24_sim-gen-decoupled.html">
     (24) Simulation – generate (fix &amp; obj uncoupled)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-02/24_sim-gen-save.html">
     (24) Simulation – generate (save)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-02/25_fit-cuda.html">
     (25) Fit – cuda (gaban)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-02/25_fit-cuda0.html">
     (25) Fit – cuda0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-02/25_fit-cuda1.html">
     (25) Fit – cuda1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-02/25_fit-cuda2.html">
     (25) Fit – cuda2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-02/30_kl-balancer-debug.html">
     (30) KL Balancer
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  March 2023
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../md/2023-03.html">
   Summary
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../md/2023-03_updates.html">
   Updates
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-03/17_sim-new-draft.html">
     (17) Sim New – draft
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-03/22_sim-new-draft.html">
     (22) Sim New – draft
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-03/23_sim-new.html">
     (23) Sim New – visualize
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-03/30_fit-cuda.html">
     (30) Fit – cuda
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-03/30_fit-cuda0.html">
     (30) Fit – cuda0
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-03/30_fit-cuda1.html">
     (30) Fit – cuda2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-03/30_fit-cuda2.html">
     (30) Fit – cuda2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-03/30_sim-final.html">
     (30) Sim – added
     <code class="docutils literal notranslate">
      <span class="pre">
       terrain
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../2023-03/31_sim-factors.html">
     (31) Sim – true generative factors
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  April 2023
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../md/2023-04.html">
   Summary
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../../md/2023-04_updates.html">
   Updates
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="02_fit-lin.html">
     (02) Fit neurons – Linear
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03_fit-glm.html">
     (03) Fit neurons – GLM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03_loop-glm.html">
     (03) Fit neurons – GLM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="03_loop-ridge.html">
     (03) Fit neurons – Ridge
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="04_dan.html">
     (04) Dan notebook
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     (04) PyTorch profiler
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="06_sim-kNN.html">
     (06) Sim – kNN analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07_readout-3d.html">
     (07) Readout: 3D conv
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="07_readout-sep.html">
     (07) Readout: Separable
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08_GLM-clu090.html">
     (08) Reservoir + GLM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08_GLM-clu096.html">
     (08) Reservoir + GLM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08_sim-dim65-manyseeds.html">
     (08) Sim – many seeds
     <code class="docutils literal notranslate">
      <span class="pre">
       dim=65
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="08_sim-accept-filter.html">
     (08) Sim – behavior of
     <code class="docutils literal notranslate">
      <span class="pre">
       filter()
      </span>
     </code>
     when
     <code class="docutils literal notranslate">
      <span class="pre">
       dim=65
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="09_sim-final-save.html">
     (09) Sim – generate &amp; save
     <code class="docutils literal notranslate">
      <span class="pre">
       dim=65
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="10_GLM-clu090.html">
     (10) Reservoir + GLM
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/nb/2023-04/04_profiler.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/hadivafaii/jb-MTMST"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/hadivafaii/jb-MTMST/issues/new?title=Issue%20on%20page%20%2Fnb/2023-04/04_profiler.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/hadivafaii/jb-MTMST/master?urlpath=tree/docs/nb/2023-04/04_profiler.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#trainer">
   Trainer
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#save-to-txt">
     Save to txt?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#compile">
   Compile
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <section id="pytorch-profiler">
<h1>(04) PyTorch profiler<a class="headerlink" href="#pytorch-profiler" title="Permalink to this headline">¶</a></h1>
<p><strong>Motivation</strong>: &#64;torch.jit.script was slowing things down <br></p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># HIDE CODE</span>


<span class="kn">import</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>

<span class="c1"># tmp &amp; extras dir</span>
<span class="n">git_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;HOME&#39;</span><span class="p">],</span> <span class="s1">&#39;Dropbox/git&#39;</span><span class="p">)</span>
<span class="n">extras_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">git_dir</span><span class="p">,</span> <span class="s1">&#39;jb-MTMST/_extras&#39;</span><span class="p">)</span>
<span class="n">fig_base_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">git_dir</span><span class="p">,</span> <span class="s1">&#39;jb-MTMST/figs&#39;</span><span class="p">)</span>
<span class="n">tmp_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">git_dir</span><span class="p">,</span> <span class="s1">&#39;jb-MTMST/tmp&#39;</span><span class="p">)</span>

<span class="c1"># GitHub</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">git_dir</span><span class="p">,</span> <span class="s1">&#39;_MTMST&#39;</span><span class="p">))</span>
<span class="kn">from</span> <span class="nn">model.train_vae</span> <span class="kn">import</span> <span class="n">TrainerVAE</span><span class="p">,</span> <span class="n">ConfigTrainVAE</span>
<span class="kn">from</span> <span class="nn">model.vae2d</span> <span class="kn">import</span> <span class="n">VAE</span><span class="p">,</span> <span class="n">ConfigVAE</span>
<span class="kn">from</span> <span class="nn">analysis.opticflow</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">figures.fighelper</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># warnings, tqdm, &amp; style</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">DeprecationWarning</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">rich.jupyter</span> <span class="kn">import</span> <span class="nb">print</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">set_style</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<section id="trainer">
<h2>Trainer<a class="headerlink" href="#trainer" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vae</span> <span class="o">=</span> <span class="n">VAE</span><span class="p">(</span><span class="n">ConfigVAE</span><span class="p">(</span>
    <span class="n">n_latent_scales</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_groups_per_scale</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_latent_per_group</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
    <span class="n">scale_init</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">residual_kl</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ada_groups</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># separable=False,</span>
<span class="p">))</span>
<span class="bp">self</span> <span class="o">=</span> <span class="n">TrainerVAE</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">vae</span><span class="p">,</span>
    <span class="n">cfg</span><span class="o">=</span><span class="n">ConfigTrainVAE</span><span class="p">(</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">0.002</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">grad_clip</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">lambda_anneal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lambda_init</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">lambda_norm</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span>
        <span class="n">kl_beta</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">kl_anneal_cycles</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
        <span class="n">scheduler_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;T_max&#39;</span><span class="p">:</span> <span class="mf">660.0</span><span class="p">,</span> <span class="s1">&#39;eta_min&#39;</span><span class="p">:</span> <span class="mf">1e-05</span><span class="p">},</span>   
        <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adamax_fast&#39;</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">vae</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">total_latents</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>210
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vae</span><span class="o">.</span><span class="n">print</span><span class="p">()</span>
<span class="n">vae</span><span class="o">.</span><span class="n">scales</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">+--------------+------------+
| Module Name  | Num Params |
+--------------+------------+
|     VAE      |  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17.0</span> Mil  |
|     ---      |    ---     |
|     stem     |   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.1</span> K    |
| pre_process  |   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">96.0</span> K   |
|  enc_tower   |  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9.4</span> Mil   |
|     enc0     |   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16.6</span> K   |
| enc_sampler  |  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.4</span> Mil   |
| dec_sampler  |  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.4</span> Mil   |
|    expand    |   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">71.0</span> K   |
|  dec_tower   |  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4.5</span> Mil   |
| post_process |   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">40.0</span> K   |
|     out      |    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">578</span>     |
+--------------+------------+ 


</pre>
</div><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[8, 4]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">vae</span><span class="o">.</span><span class="n">all_conv_layers</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">vae</span><span class="o">.</span><span class="n">all_log_norm</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(291, 287)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># neither epe, nor _normalize have jit. furthermore, _normalize uses linalg.vector_norm</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>

<span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch # 100, avg loss: 4.861284: 100%|██████████| 100/100 [1:49:38&lt;00:00, 65.79s/it]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 1h 47min 2s, sys: 3min 38s, total: 1h 50min 41s
Wall time: 1h 49min 39s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>

<span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">norm</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dl_trn</span><span class="p">)):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 80/80 [00:18&lt;00:00,  4.25it/s]
100%|██████████| 80/80 [00:16&lt;00:00,  4.93it/s]
100%|██████████| 80/80 [00:17&lt;00:00,  4.71it/s]
100%|██████████| 80/80 [00:14&lt;00:00,  5.49it/s]
100%|██████████| 80/80 [00:14&lt;00:00,  5.67it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 1min 21s, sys: 1.53 s, total: 1min 22s
Wall time: 1min 20s
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># was with bunch of jit at Normal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 80/80 [00:21&lt;00:00,  3.81it/s]
100%|██████████| 80/80 [00:15&lt;00:00,  5.09it/s]
100%|██████████| 80/80 [00:17&lt;00:00,  4.57it/s]
100%|██████████| 80/80 [00:14&lt;00:00,  5.35it/s]
100%|██████████| 80/80 [00:15&lt;00:00,  5.00it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 1min 26s, sys: 1.65 s, total: 1min 27s
Wall time: 1min 25s
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># was with torch.linalg.vector_norm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 80/80 [00:14&lt;00:00,  5.57it/s]
100%|██████████| 80/80 [00:13&lt;00:00,  6.07it/s]
100%|██████████| 80/80 [00:11&lt;00:00,  6.92it/s]
100%|██████████| 80/80 [00:12&lt;00:00,  6.27it/s]
100%|██████████| 80/80 [00:11&lt;00:00,  6.88it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 1min 4s, sys: 1.64 s, total: 1min 6s
Wall time: 1min 3s
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">model.utils_model</span> <span class="kn">import</span> <span class="n">kl_balancer</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|          | 0/10 [00:00&lt;?, ?it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span> 
    <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_iters_warmup</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>gstep # 61, nelbo: 15.907, grad: 228.0:   0%|          | 0/10 [05:06&lt;?, ?it/s]      
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">File</span> <span class="o">&lt;</span><span class="n">timed</span> <span class="n">exec</span><span class="o">&gt;</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>

<span class="nn">File ~/Dropbox/git/_MTMST/model/train_vae.py:84,</span> in <span class="ni">TrainerVAE.iteration</span><span class="nt">(self, epoch, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">82</span> <span class="c1"># forward + loss</span>
<span class="g g-Whitespace">     </span><span class="mi">83</span> <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">use_amp</span><span class="p">):</span>
<span class="ne">---&gt; </span><span class="mi">84</span> 	<span class="n">y</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">85</span> 	<span class="n">epe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">loss_recon</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">norm</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">86</span> 	<span class="n">kl_all</span><span class="p">,</span> <span class="n">kl_diag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">loss_kl</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1496</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1497</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1498</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_pre_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1499</span>         <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1500</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1501</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1502</span> <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1503</span> <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>

<span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span> 
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">norm</span><span class="p">)</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dl_trn</span><span class="p">))):</span>
        <span class="n">gstep</span> <span class="o">=</span> <span class="n">e</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dl_trn</span><span class="p">)</span> <span class="o">+</span> <span class="n">i</span>
        <span class="n">y</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">epe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">loss_recon</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">norm</span><span class="p">)</span>
        <span class="n">kl_all</span><span class="p">,</span> <span class="n">kl_diag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">loss_kl</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
        <span class="c1"># balance kl</span>
        <span class="n">balanced_kl</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">kl_vals</span> <span class="o">=</span> <span class="n">kl_balancer</span><span class="p">(</span>
            <span class="n">kl_all</span><span class="o">=</span><span class="n">kl_all</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alphas</span><span class="p">,</span>
            <span class="n">coeff</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">betas</span><span class="p">[</span><span class="n">gstep</span><span class="p">],</span>
            <span class="n">beta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">kl_beta</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">epe</span> <span class="o">+</span> <span class="n">balanced_kl</span><span class="p">)</span>
        <span class="c1"># add regularization</span>
        <span class="n">loss_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">loss_weight</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wd_coeffs</span><span class="p">[</span><span class="n">gstep</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wd_coeffs</span><span class="p">[</span><span class="n">gstep</span><span class="p">]</span> <span class="o">*</span> <span class="n">loss_w</span>
        <span class="n">cond_reg_spectral</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">lambda_norm</span> <span class="o">&gt;</span> <span class="mi">0</span> \
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">spectral_reg</span> <span class="ow">and</span> \
            <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">spectral_norm</span>
        <span class="k">if</span> <span class="n">cond_reg_spectral</span><span class="p">:</span>
            <span class="n">loss_sr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">loss_spectral</span><span class="p">(</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wd_coeffs</span><span class="p">[</span><span class="n">gstep</span><span class="p">]</span> <span class="o">*</span> <span class="n">loss_sr</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss_sr</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>80it [00:54,  1.48it/s]
80it [00:43,  1.84it/s]
80it [00:44,  1.78it/s]
80it [00:48,  1.65it/s]
80it [00:44,  1.79it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 3min 51s, sys: 6.77 s, total: 3min 58s
Wall time: 3min 55s
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># was with torch.linalg.vector_norm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>80it [00:51,  1.54it/s]
80it [00:46,  1.72it/s]
80it [00:42,  1.88it/s]
80it [00:46,  1.73it/s]
80it [00:48,  1.63it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 3min 50s, sys: 8.81 s, total: 3min 59s
Wall time: 3min 56s
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">activities</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">kws</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">activities</span><span class="o">=</span><span class="n">activities</span><span class="p">,</span>
    <span class="n">record_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">with_stack</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span><span class="o">**</span><span class="n">kws</span><span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">epe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">loss_recon</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">norm</span><span class="p">)</span>
    <span class="n">kl_all</span><span class="p">,</span> <span class="n">kl_diag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">loss_kl</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="c1"># balance kl</span>
    <span class="n">balanced_kl</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">kl_vals</span> <span class="o">=</span> <span class="n">kl_balancer</span><span class="p">(</span>
        <span class="n">kl_all</span><span class="o">=</span><span class="n">kl_all</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alphas</span><span class="p">,</span>
        <span class="n">coeff</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">betas</span><span class="p">[</span><span class="n">gstep</span><span class="p">],</span>
        <span class="n">beta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">kl_beta</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">epe</span> <span class="o">+</span> <span class="n">balanced_kl</span><span class="p">)</span>
    <span class="c1"># add regularization</span>
    <span class="n">loss_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">loss_weight</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">wd_coeffs</span><span class="p">[</span><span class="n">gstep</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wd_coeffs</span><span class="p">[</span><span class="n">gstep</span><span class="p">]</span> <span class="o">*</span> <span class="n">loss_w</span>
    <span class="n">cond_reg_spectral</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">lambda_norm</span> <span class="o">&gt;</span> <span class="mi">0</span> \
        <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">spectral_reg</span> <span class="ow">and</span> \
        <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">spectral_norm</span>
    <span class="k">if</span> <span class="n">cond_reg_spectral</span><span class="p">:</span>
        <span class="n">loss_sr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">loss_spectral</span><span class="p">(</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">wd_coeffs</span><span class="p">[</span><span class="n">gstep</span><span class="p">]</span> <span class="o">*</span> <span class="n">loss_sr</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss_sr</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">(</span><span class="n">group_by_stack_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;self_cuda_time_total&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">-------------------------------------------------------  ------------  ------------  
------------  ------------  ------------  ------------  ------------  ------------  
------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU 
total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg 
# of Calls  
-------------------------------------------------------  ------------  ------------  
------------  ------------  ------------  ------------  ------------  ------------  
------------  ------------  
                             aten::convolution_backward         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5.90</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">56.</span>383ms        
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11.40</span>%     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">108.</span>907ms     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">310.</span>276us     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">208.</span>844ms        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">49.17</span>%     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">218.</span>653ms     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">622.</span>943us   
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">351</span>  
                                aten::cudnn_convolution         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5.00</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">47.</span>745ms         
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7.45</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">71.</span>201ms     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">221.</span>810us      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">71.</span>497ms        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16.83</span>%     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">105.</span>171ms     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">327.</span>636us    
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">321</span>  
void cudnn::detail::dgrad_alg1_engine&lt;float, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">128</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>,<span style="color: #808000; text-decoration-color: #808000">...</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.00</span>%       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.</span>000us         
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.00</span>%       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.</span>000us       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.</span>000us      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">63.</span>215ms        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14.88</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">63.</span>215ms       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.</span>054ms    
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">60</span>  
                                   volta_sgemm_64x64_nt         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.00</span>%       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.</span>000us         
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.00</span>%       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.</span>000us       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.</span>000us      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">46.</span>713ms        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11.00</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">46.</span>713ms     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">126.</span>593us    
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">369</span>  
                                       cudaLaunchKernel        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">26.61</span>%     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">254.</span>222ms        
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">26.61</span>%     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">254.</span>222ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16.</span>282us      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">43.</span>567ms        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10.26</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">43.</span>567ms       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.</span>790us   
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15614</span>  
                                             aten::add_         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3.54</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">33.</span>856ms         
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6.90</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">65.</span>932ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">26.</span>205us      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32.</span>449ms         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7.64</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">34.</span>505ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13.</span>714us    
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2516</span>  
void at::native::vectorized_elementwise_kernel&lt;<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, at<span style="color: #808000; text-decoration-color: #808000">...</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.00</span>%       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.</span>000us         
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.00</span>%       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.</span>000us       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.</span>000us      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">31.</span>880ms         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7.51</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">31.</span>880ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12.</span>468us    
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2557</span>  
void cudnn::winograd_nonfused::winogradForwardData4x<span style="color: #808000; text-decoration-color: #808000">...</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.00</span>%       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.</span>000us         
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.00</span>%       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.</span>000us       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.</span>000us      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">31.</span>435ms         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7.40</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">31.</span>435ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">85.</span>190us    
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">369</span>  
void cudnn::winograd_nonfused::winogradForwardOutput<span style="color: #808000; text-decoration-color: #808000">...</span>         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.00</span>%       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.</span>000us         
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.00</span>%       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.</span>000us       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.</span>000us      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">26.</span>804ms         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6.31</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">26.</span>804ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">72.</span>640us    
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">369</span>  
                                              aten::mul         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5.81</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">55.</span>523ms         
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9.15</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">87.</span>372ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">37.</span>725us      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">21.</span>854ms         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5.15</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">26.</span>911ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11.</span>620us    
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2316</span>  
-------------------------------------------------------  ------------  ------------  
------------  ------------  ------------  ------------  ------------  ------------  
------------  ------------  
Self CPU time total: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">955.</span>230ms
Self CUDA time total: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">424.</span>738ms

</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">(</span><span class="n">group_by_stack_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;self_cpu_time_total&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">-------------------------------------------------------  ------------  ------------  
------------  ------------  ------------  ------------  ------------  ------------  
------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU 
total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg 
# of Calls  
-------------------------------------------------------  ------------  ------------  
------------  ------------  ------------  ------------  ------------  ------------  
------------  ------------  
                                       cudaLaunchKernel        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">26.61</span>%     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">254.</span>222ms        
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">26.61</span>%     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">254.</span>222ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16.</span>282us      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">43.</span>567ms        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10.26</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">43.</span>567ms       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.</span>790us   
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">15614</span>  
                             aten::convolution_backward         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5.90</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">56.</span>383ms        
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11.40</span>%     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">108.</span>907ms     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">310.</span>276us     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">208.</span>844ms        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">49.17</span>%     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">218.</span>653ms     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">622.</span>943us   
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">351</span>  
                                              aten::mul         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5.81</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">55.</span>523ms         
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9.15</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">87.</span>372ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">37.</span>725us      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">21.</span>854ms         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5.15</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">26.</span>911ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11.</span>620us    
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2316</span>  
                                aten::cudnn_convolution         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5.00</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">47.</span>745ms         
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7.45</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">71.</span>201ms     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">221.</span>810us      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">71.</span>497ms        <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">16.83</span>%     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">105.</span>171ms     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">327.</span>636us    
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">321</span>  
                                              aten::div         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4.57</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">43.</span>701ms         
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6.90</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">65.</span>879ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">39.</span>331us       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6.</span>326ms         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.49</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10.</span>167ms       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6.</span>070us    
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1675</span>  
                                             aten::add_         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3.54</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">33.</span>856ms         
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6.90</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">65.</span>932ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">26.</span>205us      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32.</span>449ms         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7.64</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">34.</span>505ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13.</span>714us    
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2516</span>  
                                              aten::sum         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3.28</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">31.</span>303ms         
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4.86</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">46.</span>424ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">36.</span>962us      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17.</span>573ms         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4.14</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17.</span>573ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13.</span>991us    
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1256</span>  
                                            aten::addmm         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.97</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">28.</span>349ms         
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4.38</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">41.</span>835ms     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">166.</span>012us       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.</span>517ms         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.36</span>%       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4.</span>990ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19.</span>802us    
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">252</span>  
                                              aten::add         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.82</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">26.</span>940ms         
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4.29</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">40.</span>992ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">51.</span>627us      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11.</span>840ms         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.79</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14.</span>414ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">18.</span>154us    
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">794</span>  
                                              aten::exp         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.09</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19.</span>971ms         
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5.51</span>%      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">52.</span>658ms     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">180.</span>336us     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">682.</span>000us         <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.16</span>%       <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6.</span>437ms      <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">22.</span>045us    
<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">292</span>  
-------------------------------------------------------  ------------  ------------  
------------  ------------  ------------  ------------  ------------  ------------  
------------  ------------  
Self CPU time total: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">955.</span>230ms
Self CUDA time total: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">424.</span>738ms

</pre>
</div></div>
</div>
<section id="save-to-txt">
<h3>Save to txt?<a class="headerlink" href="#save-to-txt" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">(</span><span class="n">group_by_stack_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;self_cuda_time_total&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;profile_cuda.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    
<span class="n">output</span> <span class="o">=</span> <span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">(</span><span class="n">group_by_stack_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;self_cpu_time_total&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;profile_cpu.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Conclusion</strong>: jit was slowing things down</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">activities</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">kws</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
    <span class="n">activities</span><span class="o">=</span><span class="n">activities</span><span class="p">,</span>
    <span class="n">record_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">with_stack</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span><span class="o">**</span><span class="n">kws</span><span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>epoch # 1, avg loss: 57.204149: 100%|██████████| 1/1 [02:20&lt;00:00, 140.16s/it]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">(</span><span class="n">group_by_stack_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;self_cuda_time_total&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;profile_cuda.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    
<span class="n">output</span> <span class="o">=</span> <span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">(</span><span class="n">group_by_stack_n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;self_cpu_time_total&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;profile_cpu.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="compile">
<h2>Compile<a class="headerlink" href="#compile" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vae</span> <span class="o">=</span> <span class="n">VAE</span><span class="p">(</span><span class="n">ConfigVAE</span><span class="p">(</span>
    <span class="n">n_latent_scales</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_groups_per_scale</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_latent_per_group</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
    <span class="n">scale_init</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">residual_kl</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ada_groups</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># separable=False,</span>
<span class="p">))</span>
<span class="n">tr</span> <span class="o">=</span> <span class="n">TrainerVAE</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">vae</span><span class="p">),</span>
    <span class="n">cfg</span><span class="o">=</span><span class="n">ConfigTrainVAE</span><span class="p">(</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">0.002</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">grad_clip</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
        <span class="n">lambda_anneal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lambda_init</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">lambda_norm</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span>
        <span class="n">kl_beta</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">kl_anneal_cycles</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
        <span class="n">scheduler_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;T_max&#39;</span><span class="p">:</span> <span class="mf">660.0</span><span class="p">,</span> <span class="s1">&#39;eta_min&#39;</span><span class="p">:</span> <span class="mf">1e-05</span><span class="p">},</span>   
        <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adamax_fast&#39;</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">vae</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">total_latents</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>210
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>

<span class="n">tr</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">norm</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">tr</span><span class="o">.</span><span class="n">dl_trn</span><span class="p">)):</span>
        <span class="n">tr</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "2e51b5f6a2624475997333e7fe69fd90", "version_major": 2, "version_minor": 0}
</script><div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">RuntimeError</span><span class="g g-Whitespace">                              </span>Traceback (most recent call last)
<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/output_graph.py:670,</span> in <span class="ni">OutputGraph.call_user_compiler</span><span class="nt">(self, gm)</span>
<span class="g g-Whitespace">    </span><span class="mi">669</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">670</span>     <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">fake_example_inputs</span><span class="p">())</span>
<span class="g g-Whitespace">    </span><span class="mi">671</span> <span class="n">_step_logger</span><span class="p">()(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;done compiler function </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/debug_utils.py:1055,</span> in <span class="ni">wrap_backend_debug.&lt;locals&gt;.debug_wrapper</span><span class="nt">(gm, example_inputs, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1054</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1055</span>     <span class="n">compiled_gm</span> <span class="o">=</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1057</span> <span class="k">return</span> <span class="n">compiled_gm</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/__init__.py:1390,</span> in <span class="ni">_TorchCompileInductorWrapper.__call__</span><span class="nt">(self, model_, inputs_)</span>
<span class="g g-Whitespace">   </span><span class="mi">1388</span> <span class="kn">from</span> <span class="nn">torch._inductor.compile_fx</span> <span class="kn">import</span> <span class="n">compile_fx</span>
<span class="ne">-&gt; </span><span class="mi">1390</span> <span class="k">return</span> <span class="n">compile_fx</span><span class="p">(</span><span class="n">model_</span><span class="p">,</span> <span class="n">inputs_</span><span class="p">,</span> <span class="n">config_patches</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_inductor/compile_fx.py:455,</span> in <span class="ni">compile_fx</span><span class="nt">(model_, example_inputs_, inner_compile, config_patches)</span>
<span class="g g-Whitespace">    </span><span class="mi">450</span> <span class="k">with</span> <span class="n">overrides</span><span class="o">.</span><span class="n">patch_functions</span><span class="p">():</span>
<span class="g g-Whitespace">    </span><span class="mi">451</span> 
<span class="g g-Whitespace">    </span><span class="mi">452</span>     <span class="c1"># TODO: can add logging before/after the call to create_aot_dispatcher_function</span>
<span class="g g-Whitespace">    </span><span class="mi">453</span>     <span class="c1"># in torch._functorch/aot_autograd.py::aot_module_simplified::aot_function_simplified::new_func</span>
<span class="g g-Whitespace">    </span><span class="mi">454</span>     <span class="c1"># once torchdynamo is merged into pytorch</span>
<span class="ne">--&gt; </span><span class="mi">455</span>     <span class="k">return</span> <span class="n">aot_autograd</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">456</span>         <span class="n">fw_compiler</span><span class="o">=</span><span class="n">fw_compiler</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">457</span>         <span class="n">bw_compiler</span><span class="o">=</span><span class="n">bw_compiler</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">458</span>         <span class="n">decompositions</span><span class="o">=</span><span class="n">select_decomp_table</span><span class="p">(),</span>
<span class="g g-Whitespace">    </span><span class="mi">459</span>         <span class="n">partition_fn</span><span class="o">=</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">460</span>             <span class="n">min_cut_rematerialization_partition</span><span class="p">,</span> <span class="n">compiler</span><span class="o">=</span><span class="s2">&quot;inductor&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">461</span>         <span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">462</span>         <span class="n">keep_inference_input_mutations</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">463</span>     <span class="p">)(</span><span class="n">model_</span><span class="p">,</span> <span class="n">example_inputs_</span><span class="p">)</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/backends/common.py:48,</span> in <span class="ni">aot_autograd.&lt;locals&gt;.compiler_fn</span><span class="nt">(gm, example_inputs)</span>
<span class="g g-Whitespace">     </span><span class="mi">47</span> <span class="k">with</span> <span class="n">enable_aot_logging</span><span class="p">():</span>
<span class="ne">---&gt; </span><span class="mi">48</span>     <span class="n">cg</span> <span class="o">=</span> <span class="n">aot_module_simplified</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">example_inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">49</span>     <span class="n">counters</span><span class="p">[</span><span class="s2">&quot;aot_autograd&quot;</span><span class="p">][</span><span class="s2">&quot;ok&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py:2805,</span> in <span class="ni">aot_module_simplified</span><span class="nt">(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, hasher_type, static_argnums, keep_inference_input_mutations)</span>
<span class="g g-Whitespace">   </span><span class="mi">2803</span> <span class="n">full_args</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">2805</span> <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">create_aot_dispatcher_function</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">2806</span>     <span class="n">functional_call</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2807</span>     <span class="n">full_args</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2808</span>     <span class="n">aot_config</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">2809</span> <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2811</span> <span class="c1"># TODO: There is something deeply wrong here; compiled_fn running with</span>
<span class="g g-Whitespace">   </span><span class="mi">2812</span> <span class="c1"># the boxed calling convention, but aot_module_simplified somehow</span>
<span class="g g-Whitespace">   </span><span class="mi">2813</span> <span class="c1"># historically returned a function that was not the boxed calling</span>
<span class="g g-Whitespace">   </span><span class="mi">2814</span> <span class="c1"># convention.  This should get fixed...</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/utils.py:163,</span> in <span class="ni">dynamo_timed.&lt;locals&gt;.dynamo_timed_inner.&lt;locals&gt;.time_wrapper</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">162</span> <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="ne">--&gt; </span><span class="mi">163</span> <span class="n">r</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">164</span> <span class="n">time_spent</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py:2498,</span> in <span class="ni">create_aot_dispatcher_function</span><span class="nt">(flat_fn, flat_args, aot_config)</span>
<span class="g g-Whitespace">   </span><span class="mi">2496</span> <span class="c1"># You can put more passes here</span>
<span class="ne">-&gt; </span><span class="mi">2498</span> <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">fake_flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2500</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">,</span> <span class="s2">&quot;_boxed_call&quot;</span><span class="p">):</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py:1713,</span> in <span class="ni">aot_wrapper_dedupe</span><span class="nt">(flat_fn, flat_args, aot_config, compiler_fn)</span>
<span class="g g-Whitespace">   </span><span class="mi">1712</span>     <span class="k">if</span> <span class="n">ok</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1713</span>         <span class="k">return</span> <span class="n">compiler_fn</span><span class="p">(</span><span class="n">flat_fn</span><span class="p">,</span> <span class="n">leaf_flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1715</span> <span class="c1"># Strategy 2: Duplicate specialize.</span>
<span class="g g-Whitespace">   </span><span class="mi">1716</span> <span class="c1">#</span>
<span class="g g-Whitespace">   </span><span class="mi">1717</span> <span class="c1"># In Haskell types, suppose you have:</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1749</span> <span class="c1">#   }</span>
<span class="g g-Whitespace">   </span><span class="mi">1750</span> <span class="c1">#   keep_arg_mask = [True, True, False, True]</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py:2087,</span> in <span class="ni">aot_dispatch_autograd</span><span class="nt">(flat_fn, flat_args, aot_config)</span>
<span class="g g-Whitespace">   </span><span class="mi">2086</span>     <span class="n">flattened_joints</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">joint_inputs</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">2087</span>     <span class="n">fx_g</span> <span class="o">=</span> <span class="n">make_fx</span><span class="p">(</span><span class="n">joint_forward_backward</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">.</span><span class="n">decompositions</span><span class="p">)(</span>
<span class="g g-Whitespace">   </span><span class="mi">2088</span>         <span class="o">*</span><span class="n">joint_inputs</span>
<span class="g g-Whitespace">   </span><span class="mi">2089</span>     <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2091</span> <span class="c1"># There should be *NO* mutating ops in the graph at this point.</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/fx/experimental/proxy_tensor.py:714,</span> in <span class="ni">make_fx.&lt;locals&gt;.wrapped</span><span class="nt">(*args)</span>
<span class="g g-Whitespace">    </span><span class="mi">712</span> <span class="k">with</span> <span class="n">decompose</span><span class="p">(</span><span class="n">decomposition_table</span><span class="p">),</span> <span class="n">fake_tensor_mode</span><span class="p">,</span> <span class="n">python_dispatcher_mode</span><span class="p">,</span> \
<span class="g g-Whitespace">    </span><span class="mi">713</span>      <span class="n">sym_mode</span><span class="p">,</span> <span class="n">proxy_mode</span><span class="p">,</span> <span class="n">disable_autocast_cache</span><span class="p">(),</span> <span class="n">disable_proxy_modes_tracing</span><span class="p">(</span><span class="n">enable_current</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">714</span>     <span class="n">t</span> <span class="o">=</span> <span class="n">dispatch_trace</span><span class="p">(</span><span class="n">wrap_key</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">fx_tracer</span><span class="p">),</span> <span class="n">tracer</span><span class="o">=</span><span class="n">fx_tracer</span><span class="p">,</span> <span class="n">concrete_args</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">phs</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">716</span> <span class="c1"># TODO: kind of a bad way to do it, should maybe figure out a better way</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:209,</span> in <span class="ni">_TorchDynamoContext.__call__.&lt;locals&gt;._fn</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">208</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">209</span>     <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">210</span> <span class="k">finally</span><span class="p">:</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/fx/experimental/proxy_tensor.py:443,</span> in <span class="ni">dispatch_trace</span><span class="nt">(root, tracer, concrete_args)</span>
<span class="g g-Whitespace">    </span><span class="mi">438</span> <span class="k">def</span> <span class="nf">dispatch_trace</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">439</span>         <span class="n">root</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Callable</span><span class="p">],</span>
<span class="g g-Whitespace">    </span><span class="mi">440</span>         <span class="n">tracer</span><span class="p">:</span> <span class="n">Tracer</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">441</span>         <span class="n">concrete_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">442</span> <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GraphModule</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">443</span>     <span class="n">graph</span> <span class="o">=</span> <span class="n">tracer</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">concrete_args</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">444</span>     <span class="n">name</span> <span class="o">=</span> <span class="n">root</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="k">else</span> <span class="n">root</span><span class="o">.</span><span class="vm">__name__</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:209,</span> in <span class="ni">_TorchDynamoContext.__call__.&lt;locals&gt;._fn</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">208</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">209</span>     <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">210</span> <span class="k">finally</span><span class="p">:</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/fx/_symbolic_trace.py:778,</span> in <span class="ni">Tracer.trace</span><span class="nt">(self, root, concrete_args)</span>
<span class="g g-Whitespace">    </span><span class="mi">772</span>         <span class="n">_autowrap_check</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">773</span>             <span class="n">patcher</span><span class="p">,</span> <span class="n">module</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_autowrap_function_ids</span>
<span class="g g-Whitespace">    </span><span class="mi">774</span>         <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">775</span>     <span class="bp">self</span><span class="o">.</span><span class="n">create_node</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">776</span>         <span class="s2">&quot;output&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">777</span>         <span class="s2">&quot;output&quot;</span><span class="p">,</span>
<span class="ne">--&gt; </span><span class="mi">778</span>         <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">create_arg</span><span class="p">(</span><span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)),),</span>
<span class="g g-Whitespace">    </span><span class="mi">779</span>         <span class="p">{},</span>
<span class="g g-Whitespace">    </span><span class="mi">780</span>         <span class="n">type_expr</span><span class="o">=</span><span class="n">fn</span><span class="o">.</span><span class="vm">__annotations__</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;return&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">781</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">783</span> <span class="bp">self</span><span class="o">.</span><span class="n">submodule_paths</span> <span class="o">=</span> <span class="kc">None</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/fx/_symbolic_trace.py:652,</span> in <span class="ni">Tracer.create_args_for_root.&lt;locals&gt;.flatten_fn</span><span class="nt">(*args)</span>
<span class="g g-Whitespace">    </span><span class="mi">651</span> <span class="n">tree_args</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_unflatten</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">),</span> <span class="n">in_spec</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">652</span> <span class="n">tree_out</span> <span class="o">=</span> <span class="n">root_fn</span><span class="p">(</span><span class="o">*</span><span class="n">tree_args</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">653</span> <span class="n">out_args</span><span class="p">,</span> <span class="n">out_spec</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_flatten</span><span class="p">(</span><span class="n">tree_out</span><span class="p">)</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/fx/experimental/proxy_tensor.py:459,</span> in <span class="ni">wrap_key.&lt;locals&gt;.wrapped</span><span class="nt">(*proxies)</span>
<span class="g g-Whitespace">    </span><span class="mi">457</span>     <span class="n">track_tensor_tree</span><span class="p">(</span><span class="n">flat_tensors</span><span class="p">,</span> <span class="n">flat_proxies</span><span class="p">,</span> <span class="n">constant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tracer</span><span class="o">=</span><span class="n">tracer</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">459</span> <span class="n">out</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">tensors</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">460</span> <span class="n">out</span> <span class="o">=</span> <span class="n">pytree</span><span class="o">.</span><span class="n">tree_map_only</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">461</span>     <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">462</span>     <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">get_proxy_slot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">tracer</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">proxy</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">463</span>     <span class="n">out</span>
<span class="g g-Whitespace">    </span><span class="mi">464</span> <span class="p">)</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py:1156,</span> in <span class="ni">create_forward_or_joint_functionalized.&lt;locals&gt;.traced_joint</span><span class="nt">(primals, tangents)</span>
<span class="g g-Whitespace">   </span><span class="mi">1155</span> <span class="k">def</span> <span class="nf">traced_joint</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1156</span>     <span class="k">return</span> <span class="n">functionalized_f_helper</span><span class="p">(</span><span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">)</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py:1108,</span> in <span class="ni">create_forward_or_joint_functionalized.&lt;locals&gt;.functionalized_f_helper</span><span class="nt">(primals, maybe_tangents)</span>
<span class="g g-Whitespace">   </span><span class="mi">1106</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1107</span>     <span class="c1"># Run the joint</span>
<span class="ne">-&gt; </span><span class="mi">1108</span>     <span class="n">f_outs</span> <span class="o">=</span> <span class="n">flat_fn_no_input_mutations</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">f_primals</span><span class="p">,</span> <span class="n">f_tangents</span><span class="p">,</span> <span class="n">meta</span><span class="p">,</span> <span class="n">keep_input_mutations</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1109</span> <span class="k">finally</span><span class="p">:</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py:1076,</span> in <span class="ni">flat_fn_no_input_mutations</span><span class="nt">(fn, primals, maybe_tangents, meta, keep_input_mutations)</span>
<span class="g g-Whitespace">   </span><span class="mi">1075</span>     <span class="n">primals_after_cloning</span> <span class="o">=</span> <span class="n">primals</span>
<span class="ne">-&gt; </span><span class="mi">1076</span> <span class="n">outs</span> <span class="o">=</span> <span class="n">flat_fn_with_synthetic_bases_expanded</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">primals_after_cloning</span><span class="p">,</span> <span class="n">maybe_tangents</span><span class="p">,</span> <span class="n">meta</span><span class="p">,</span> <span class="n">keep_input_mutations</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1077</span> <span class="k">return</span> <span class="n">outs</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py:1048,</span> in <span class="ni">flat_fn_with_synthetic_bases_expanded</span><span class="nt">(fn, primals_before_cloning, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)</span>
<span class="g g-Whitespace">   </span><span class="mi">1047</span> <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">fw_metadata</span><span class="o">.</span><span class="n">input_info</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">primals</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1048</span> <span class="n">outs</span> <span class="o">=</span> <span class="n">forward_or_joint</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">primals_before_cloning</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">maybe_tangents</span><span class="p">,</span> <span class="n">meta</span><span class="p">,</span> <span class="n">keep_input_mutations</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1049</span> <span class="k">return</span> <span class="n">outs</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_functorch/aot_autograd.py:1017,</span> in <span class="ni">forward_or_joint</span><span class="nt">(fn, primals_before_cloning, primals_after_cloning, maybe_tangents, meta, keep_input_mutations)</span>
<span class="g g-Whitespace">   </span><span class="mi">1016</span>     <span class="k">with</span> <span class="n">fx_traceback</span><span class="o">.</span><span class="n">preserve_node_meta</span><span class="p">():</span>
<span class="ne">-&gt; </span><span class="mi">1017</span>         <span class="n">backward_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1018</span>             <span class="n">needed_outs</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1019</span>             <span class="n">grad_primals</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1020</span>             <span class="n">grad_outputs</span><span class="o">=</span><span class="n">needed_tangents</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1021</span>             <span class="n">allow_unused</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1022</span>         <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1023</span> <span class="n">backward_out_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">backward_out</span><span class="p">)</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:269,</span> in <span class="ni">grad</span><span class="nt">(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)</span>
<span class="g g-Whitespace">    </span><span class="mi">268</span> <span class="k">if</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">overridable_args</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">269</span>     <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">270</span>         <span class="n">grad</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">271</span>         <span class="n">overridable_args</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">272</span>         <span class="n">t_outputs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">273</span>         <span class="n">t_inputs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">274</span>         <span class="n">grad_outputs</span><span class="o">=</span><span class="n">grad_outputs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">275</span>         <span class="n">retain_graph</span><span class="o">=</span><span class="n">retain_graph</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">276</span>         <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">277</span>         <span class="n">only_inputs</span><span class="o">=</span><span class="n">only_inputs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">278</span>         <span class="n">allow_unused</span><span class="o">=</span><span class="n">allow_unused</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">279</span>         <span class="n">is_grads_batched</span><span class="o">=</span><span class="n">is_grads_batched</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">280</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">282</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">only_inputs</span><span class="p">:</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/overrides.py:1534,</span> in <span class="ni">handle_torch_function</span><span class="nt">(public_api, relevant_args, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1533</span> <span class="k">with</span> <span class="n">_pop_mode_temporarily</span><span class="p">()</span> <span class="k">as</span> <span class="n">mode</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1534</span>     <span class="n">result</span> <span class="o">=</span> <span class="n">mode</span><span class="o">.</span><span class="n">__torch_function__</span><span class="p">(</span><span class="n">public_api</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1535</span> <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">NotImplemented</span><span class="p">:</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_inductor/overrides.py:38,</span> in <span class="ni">AutogradMonkeypatch.__torch_function__</span><span class="nt">(self, func, types, args, kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">37</span>     <span class="k">return</span> <span class="n">replacements</span><span class="p">[</span><span class="n">func</span><span class="p">](</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">38</span> <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:303,</span> in <span class="ni">grad</span><span class="nt">(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)</span>
<span class="g g-Whitespace">    </span><span class="mi">302</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">303</span>     <span class="k">return</span> <span class="n">Variable</span><span class="o">.</span><span class="n">_execution_engine</span><span class="o">.</span><span class="n">run_backward</span><span class="p">(</span>  <span class="c1"># Calls into the C++ engine to run the backward pass</span>
<span class="g g-Whitespace">    </span><span class="mi">304</span>         <span class="n">t_outputs</span><span class="p">,</span> <span class="n">grad_outputs_</span><span class="p">,</span> <span class="n">retain_graph</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="n">t_inputs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">305</span>         <span class="n">allow_unused</span><span class="p">,</span> <span class="n">accumulate_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="ne">RuntimeError</span>: Function ConvolutionBackward0 returned an invalid gradient at index 1 - expected device cuda:1 but got cuda:0

<span class="n">The</span> <span class="n">above</span> <span class="n">exception</span> <span class="n">was</span> <span class="n">the</span> <span class="n">direct</span> <span class="n">cause</span> <span class="n">of</span> <span class="n">the</span> <span class="n">following</span> <span class="n">exception</span><span class="p">:</span>

<span class="ne">BackendCompilerFailed</span><span class="g g-Whitespace">                     </span>Traceback (most recent call last)
<span class="n">File</span> <span class="o">&lt;</span><span class="n">timed</span> <span class="n">exec</span><span class="o">&gt;</span><span class="p">:</span><span class="mi">5</span><span class="p">,</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1496</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1497</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1498</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_pre_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1499</span>         <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1500</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1501</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1502</span> <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1503</span> <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:82,</span> in <span class="ni">OptimizedModule.forward</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">81</span> <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="ne">---&gt; </span><span class="mi">82</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dynamo_ctx</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_orig_mod</span><span class="o">.</span><span class="n">forward</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:209,</span> in <span class="ni">_TorchDynamoContext.__call__.&lt;locals&gt;._fn</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">207</span> <span class="n">dynamic_ctx</span><span class="o">.</span><span class="fm">__enter__</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">208</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">209</span>     <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">210</span> <span class="k">finally</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">211</span>     <span class="n">set_eval_frame</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>

<span class="nn">File ~/Dropbox/git/_MTMST/model/vae2d.py:77,</span> in <span class="ni">VAE.forward</span><span class="nt">(self, x)</span>
<span class="g g-Whitespace">     </span><span class="mi">75</span> 		<span class="n">latents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">76</span> 	<span class="c1"># &#39;combiner_dec&#39;</span>
<span class="ne">---&gt; </span><span class="mi">77</span> 	<span class="n">s</span> <span class="o">=</span> <span class="n">cell</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand</span><span class="p">[</span><span class="n">idx</span><span class="p">](</span><span class="n">z</span><span class="p">))</span>
<span class="g g-Whitespace">     </span><span class="mi">78</span> 	<span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="g g-Whitespace">     </span><span class="mi">79</span> <span class="k">else</span><span class="p">:</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1496</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1497</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1498</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_pre_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1499</span>         <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1500</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1501</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1502</span> <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1503</span> <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:337,</span> in <span class="ni">catch_errors_wrapper.&lt;locals&gt;.catch_errors</span><span class="nt">(frame, cache_size)</span>
<span class="g g-Whitespace">    </span><span class="mi">334</span>             <span class="k">return</span> <span class="n">hijacked_callback</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cache_size</span><span class="p">,</span> <span class="n">hooks</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">336</span> <span class="k">with</span> <span class="n">compile_lock</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">337</span>     <span class="k">return</span> <span class="n">callback</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cache_size</span><span class="p">,</span> <span class="n">hooks</span><span class="p">)</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py:404,</span> in <span class="ni">convert_frame.&lt;locals&gt;._convert_frame</span><span class="nt">(frame, cache_size, hooks)</span>
<span class="g g-Whitespace">    </span><span class="mi">402</span> <span class="n">counters</span><span class="p">[</span><span class="s2">&quot;frames&quot;</span><span class="p">][</span><span class="s2">&quot;total&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="g g-Whitespace">    </span><span class="mi">403</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">404</span>     <span class="n">result</span> <span class="o">=</span> <span class="n">inner_convert</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cache_size</span><span class="p">,</span> <span class="n">hooks</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">405</span>     <span class="n">counters</span><span class="p">[</span><span class="s2">&quot;frames&quot;</span><span class="p">][</span><span class="s2">&quot;ok&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="g g-Whitespace">    </span><span class="mi">406</span>     <span class="k">return</span> <span class="n">result</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py:104,</span> in <span class="ni">wrap_convert_context.&lt;locals&gt;._fn</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">102</span> <span class="n">torch</span><span class="o">.</span><span class="n">fx</span><span class="o">.</span><span class="n">graph_module</span><span class="o">.</span><span class="n">_forward_from_src</span> <span class="o">=</span> <span class="n">fx_forward_from_src_skip_result</span>
<span class="g g-Whitespace">    </span><span class="mi">103</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">104</span>     <span class="k">return</span> <span class="n">fn</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">105</span> <span class="k">finally</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">106</span>     <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_set_grad_enabled</span><span class="p">(</span><span class="n">prior_grad_mode</span><span class="p">)</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py:262,</span> in <span class="ni">convert_frame_assert.&lt;locals&gt;._convert_frame_assert</span><span class="nt">(frame, cache_size, hooks)</span>
<span class="g g-Whitespace">    </span><span class="mi">259</span> <span class="k">global</span> <span class="n">initial_grad_state</span>
<span class="g g-Whitespace">    </span><span class="mi">260</span> <span class="n">initial_grad_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_grad_enabled</span><span class="p">()</span>
<span class="ne">--&gt; </span><span class="mi">262</span> <span class="k">return</span> <span class="n">_compile</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">263</span>     <span class="n">frame</span><span class="o">.</span><span class="n">f_code</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">264</span>     <span class="n">frame</span><span class="o">.</span><span class="n">f_globals</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">265</span>     <span class="n">frame</span><span class="o">.</span><span class="n">f_locals</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">266</span>     <span class="n">frame</span><span class="o">.</span><span class="n">f_builtins</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">267</span>     <span class="n">compiler_fn</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">268</span>     <span class="n">one_graph</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">269</span>     <span class="n">export</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">270</span>     <span class="n">hooks</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">271</span>     <span class="n">frame</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">272</span> <span class="p">)</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/utils.py:163,</span> in <span class="ni">dynamo_timed.&lt;locals&gt;.dynamo_timed_inner.&lt;locals&gt;.time_wrapper</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">161</span>     <span class="n">compilation_metrics</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
<span class="g g-Whitespace">    </span><span class="mi">162</span> <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="ne">--&gt; </span><span class="mi">163</span> <span class="n">r</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">164</span> <span class="n">time_spent</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
<span class="g g-Whitespace">    </span><span class="mi">165</span> <span class="c1"># print(f&quot;Dynamo timer: key={key}, latency={latency:.2f} sec&quot;)</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py:324,</span> in <span class="ni">_compile</span><span class="nt">(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)</span>
<span class="g g-Whitespace">    </span><span class="mi">322</span> <span class="k">for</span> <span class="n">attempt</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">count</span><span class="p">():</span>
<span class="g g-Whitespace">    </span><span class="mi">323</span>     <span class="k">try</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">324</span>         <span class="n">out_code</span> <span class="o">=</span> <span class="n">transform_code_object</span><span class="p">(</span><span class="n">code</span><span class="p">,</span> <span class="n">transform</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">325</span>         <span class="n">orig_code_map</span><span class="p">[</span><span class="n">out_code</span><span class="p">]</span> <span class="o">=</span> <span class="n">code</span>
<span class="g g-Whitespace">    </span><span class="mi">326</span>         <span class="k">break</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/bytecode_transformation.py:445,</span> in <span class="ni">transform_code_object</span><span class="nt">(code, transformations, safe)</span>
<span class="g g-Whitespace">    </span><span class="mi">442</span> <span class="n">instructions</span> <span class="o">=</span> <span class="n">cleaned_instructions</span><span class="p">(</span><span class="n">code</span><span class="p">,</span> <span class="n">safe</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">443</span> <span class="n">propagate_line_nums</span><span class="p">(</span><span class="n">instructions</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">445</span> <span class="n">transformations</span><span class="p">(</span><span class="n">instructions</span><span class="p">,</span> <span class="n">code_options</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">446</span> <span class="k">return</span> <span class="n">clean_and_assemble_instructions</span><span class="p">(</span><span class="n">instructions</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">code_options</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py:311,</span> in <span class="ni">_compile.&lt;locals&gt;.transform</span><span class="nt">(instructions, code_options)</span>
<span class="g g-Whitespace">    </span><span class="mi">298</span> <span class="k">nonlocal</span> <span class="n">output</span>
<span class="g g-Whitespace">    </span><span class="mi">299</span> <span class="n">tracer</span> <span class="o">=</span> <span class="n">InstructionTranslator</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">300</span>     <span class="n">instructions</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">301</span>     <span class="n">code</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">309</span>     <span class="n">mutated_closure_cell_contents</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">310</span> <span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">311</span> <span class="n">tracer</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">312</span> <span class="n">output</span> <span class="o">=</span> <span class="n">tracer</span><span class="o">.</span><span class="n">output</span>
<span class="g g-Whitespace">    </span><span class="mi">313</span> <span class="k">assert</span> <span class="n">output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py:1726,</span> in <span class="ni">InstructionTranslator.run</span><span class="nt">(self)</span>
<span class="g g-Whitespace">   </span><span class="mi">1724</span> <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="g g-Whitespace">   </span><span class="mi">1725</span>     <span class="n">_step_logger</span><span class="p">()(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;torchdynamo start tracing </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">f_code</span><span class="o">.</span><span class="n">co_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1726</span>     <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py:576,</span> in <span class="ni">InstructionTranslatorBase.run</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">571</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">572</span>     <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">push_tx</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">573</span>     <span class="k">while</span> <span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">574</span>         <span class="bp">self</span><span class="o">.</span><span class="n">instruction_pointer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span class="g g-Whitespace">    </span><span class="mi">575</span>         <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">should_exit</span>
<span class="ne">--&gt; </span><span class="mi">576</span>         <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">577</span>     <span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">578</span>         <span class="k">pass</span>
<span class="g g-Whitespace">    </span><span class="mi">579</span> <span class="k">except</span> <span class="n">BackendCompilerFailed</span><span class="p">:</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py:540,</span> in <span class="ni">InstructionTranslatorBase.step</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">538</span>     <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inst</span><span class="o">.</span><span class="n">opname</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">539</span>         <span class="n">unimplemented</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;missing: </span><span class="si">{</span><span class="n">inst</span><span class="o">.</span><span class="n">opname</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">540</span>     <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inst</span><span class="o">.</span><span class="n">opname</span><span class="p">)(</span><span class="n">inst</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">542</span>     <span class="k">return</span> <span class="n">inst</span><span class="o">.</span><span class="n">opname</span> <span class="o">!=</span> <span class="s2">&quot;RETURN_VALUE&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">543</span> <span class="k">except</span> <span class="n">BackendCompilerFailed</span><span class="p">:</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py:1792,</span> in <span class="ni">InstructionTranslator.RETURN_VALUE</span><span class="nt">(self, inst)</span>
<span class="g g-Whitespace">   </span><span class="mi">1787</span> <span class="n">_step_logger</span><span class="p">()(</span>
<span class="g g-Whitespace">   </span><span class="mi">1788</span>     <span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1789</span>     <span class="sa">f</span><span class="s2">&quot;torchdynamo done tracing </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">f_code</span><span class="o">.</span><span class="n">co_name</span><span class="si">}</span><span class="s2"> (RETURN_VALUE)&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1790</span> <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1791</span> <span class="n">log</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;RETURN_VALUE triggered compile&quot;</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1792</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">compile_subgraph</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1793</span>     <span class="bp">self</span><span class="p">,</span> <span class="n">reason</span><span class="o">=</span><span class="n">GraphCompileReason</span><span class="p">(</span><span class="s2">&quot;return_value&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">frame_summary</span><span class="p">()])</span>
<span class="g g-Whitespace">   </span><span class="mi">1794</span> <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1795</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">add_output_instructions</span><span class="p">([</span><span class="n">create_instruction</span><span class="p">(</span><span class="s2">&quot;RETURN_VALUE&quot;</span><span class="p">)])</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/output_graph.py:541,</span> in <span class="ni">OutputGraph.compile_subgraph</span><span class="nt">(self, tx, partial_convert, reason)</span>
<span class="g g-Whitespace">    </span><span class="mi">538</span> <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
<span class="g g-Whitespace">    </span><span class="mi">539</span> <span class="k">if</span> <span class="n">count_calls</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">pass2</span><span class="o">.</span><span class="n">graph_outputs</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">540</span>     <span class="n">output</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
<span class="ne">--&gt; </span><span class="mi">541</span>         <span class="bp">self</span><span class="o">.</span><span class="n">compile_and_call_fx_graph</span><span class="p">(</span><span class="n">tx</span><span class="p">,</span> <span class="n">pass2</span><span class="o">.</span><span class="n">graph_output_vars</span><span class="p">(),</span> <span class="n">root</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">542</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">544</span>     <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pass2</span><span class="o">.</span><span class="n">graph_outputs</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">545</span>         <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pass2</span><span class="o">.</span><span class="n">create_store</span><span class="p">(</span><span class="n">graph_output_var</span><span class="p">))</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/output_graph.py:588,</span> in <span class="ni">OutputGraph.compile_and_call_fx_graph</span><span class="nt">(self, tx, rv, root)</span>
<span class="g g-Whitespace">    </span><span class="mi">586</span> <span class="n">assert_no_fake_params_or_buffers</span><span class="p">(</span><span class="n">gm</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">587</span> <span class="k">with</span> <span class="n">tracing</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tracing_context</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">588</span>     <span class="n">compiled_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">call_user_compiler</span><span class="p">(</span><span class="n">gm</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">589</span> <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">disable</span><span class="p">(</span><span class="n">compiled_fn</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">591</span> <span class="n">counters</span><span class="p">[</span><span class="s2">&quot;stats&quot;</span><span class="p">][</span><span class="s2">&quot;unique_graphs&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/utils.py:163,</span> in <span class="ni">dynamo_timed.&lt;locals&gt;.dynamo_timed_inner.&lt;locals&gt;.time_wrapper</span><span class="nt">(*args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">161</span>     <span class="n">compilation_metrics</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
<span class="g g-Whitespace">    </span><span class="mi">162</span> <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="ne">--&gt; </span><span class="mi">163</span> <span class="n">r</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">164</span> <span class="n">time_spent</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
<span class="g g-Whitespace">    </span><span class="mi">165</span> <span class="c1"># print(f&quot;Dynamo timer: key={key}, latency={latency:.2f} sec&quot;)</span>

<span class="nn">File ~/anaconda3/lib/python3.8/site-packages/torch/_dynamo/output_graph.py:675,</span> in <span class="ni">OutputGraph.call_user_compiler</span><span class="nt">(self, gm)</span>
<span class="g g-Whitespace">    </span><span class="mi">673</span> <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">674</span>     <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">gm</span><span class="o">.</span><span class="n">forward</span>
<span class="ne">--&gt; </span><span class="mi">675</span>     <span class="k">raise</span> <span class="n">BackendCompilerFailed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compiler_fn</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">e</span>
<span class="g g-Whitespace">    </span><span class="mi">676</span> <span class="k">return</span> <span class="n">compiled_fn</span>

<span class="ne">BackendCompilerFailed</span>: debug_wrapper raised RuntimeError: Function ConvolutionBackward0 returned an invalid gradient at index 1 - expected device cuda:1 but got cuda:0

<span class="n">Set</span> <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span> <span class="k">for</span> <span class="n">more</span> <span class="n">information</span>


<span class="n">You</span> <span class="n">can</span> <span class="n">suppress</span> <span class="n">this</span> <span class="n">exception</span> <span class="ow">and</span> <span class="n">fall</span> <span class="n">back</span> <span class="n">to</span> <span class="n">eager</span> <span class="n">by</span> <span class="n">setting</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_dynamo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">suppress_errors</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tr</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">comment</span><span class="o">=</span><span class="s1">&#39;compiled_test&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./nb/2023-04"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="04_dan.html" title="previous page">(04) Dan notebook</a>
    <a class='right-next' id="next-link" href="06_sim-kNN.html" title="next page">(06) Sim – kNN analysis</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Hadi Vafaii<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>