Summary
===============================

Summary of progress in March 2023

- March was the month of learning how a VAE behaves, and getting used to the difficulty of training one.
- Many things failed but I recovered. For instance:
    - High learning rate is bad (0.001 is good, maybe 0.003, but higher will hurt)
    - Spectral norm and regularization not needed, could get away with it and seems to improve fits
    - Taking out softclamp from mu and normalizing the DeConv2D layers in self.expand helped a lot.
    - Taking out all other terms in the reconstruction loss (l1, l2, cos) helped. There is no logical reason to include them. Sure, taking out l1 lead to reduced reconstruction quality, but it increased the performance of the model in predicting ground truth labels.
- Gradient clipping is necessary. I use clip by norm, but still not sure if an additional clip by value before that is helpful.
- The decoder features look a lot like encoder features, Jake's intuition was right! I expect they would both fit neurons well.
    - Next idea to try if there is time: weight sharing between the cells in enc/dec towers.
    - You can share weights, but increase the number of cells. This will surely benefit the model.